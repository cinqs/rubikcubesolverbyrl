{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/songci/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from pycuber_sc import Cube\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import itertools\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [30, 10]\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read this http://rubiks.wikia.com/wiki/Notation\n",
    "action_space = ['u', 'd', 'l', 'r', 'f', 'b', 'u\\'', 'd\\'', 'l\\'', 'r\\'', 'f\\'', 'b\\'']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self, tweaked_times_limit=100, tweaked_times=1):\n",
    "        \"\"\"\n",
    "        @param tweaked_times_limit 放弃旋转之前尝试的次数\n",
    "        @param tweaked_times 初始化一个魔方前旋转的次数\n",
    "        \"\"\"\n",
    "        self._cube = Cube()\n",
    "        # 随机初始化一个魔方\n",
    "        for _ in range(tweaked_times):\n",
    "            self._cube(str(np.random.choice(action_space)))\n",
    "        self.reset()\n",
    "        self.nA = len(action_space)\n",
    "        self.nS = len(self._get_state())\n",
    "        self._tweaked_times_limit = tweaked_times_limit\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        重置这个环境的时候将魔方恢复到初始状态\n",
    "        \"\"\"\n",
    "        self.cube = self._cube.copy()\n",
    "        self.tweaked_times = 0\n",
    "        return self._get_state()\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.cube(str(action_space[action]))\n",
    "        self.tweaked_times += 1\n",
    "        \n",
    "        done = self.cube.check() or self.tweaked_times > self._tweaked_times_limit #完成或者放弃\n",
    "        \n",
    "        reward = -0.1 if not self.cube.check() else 1 # -0.1是生存的代价，负值保证了智能体不会在收集芝麻的道路上乐此不疲\n",
    "        return self._get_state(), reward, done, None            \n",
    "            \n",
    "    def _get_state(self):\n",
    "        \"\"\"\n",
    "        将状态归一化之后，重组为元组类型\n",
    "        \"\"\"\n",
    "        return tuple(s / 5. for s in self.cube.get_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q = defaultdict(lambda :np.zeros(len(action_space)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dump([{'key': key, 'value': list(value)} for key, value in Q.items()], open('./Q.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q_ = Q.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q = Q_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "lines = 0\n",
    "with open('./Q.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        lines += 1\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reload_Q():\n",
    "    Q = defaultdict(lambda :np.zeros(len(action_space)))\n",
    "    Q_stage = json.load(open('./Q.json', 'r'))\n",
    "    for pair in Q_stage:\n",
    "        Q[tuple(pair['key'])] = np.array(pair['value'])\n",
    "    return Q\n",
    "\n",
    "Q = _reload_Q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env()\n",
    "nA = env.nA\n",
    "nS = env.nS\n",
    "with open('./Q.csv', 'w') as f:\n",
    "    header = ','.join(['s{}'.format(k_) for k_ in range(nS)]) + ',' \\\n",
    "                + ','.join(['v{}'.format(k_) for k_ in range(nA)]) + '\\n'\n",
    "    f.write(header)\n",
    "    \n",
    "    _Q = {}\n",
    "    \n",
    "    for times in range(6):\n",
    "        for _ in range(5 * 6 ** times):\n",
    "            env = Env(tweaked_times=times)\n",
    "            state = env.reset()\n",
    "            _Q[state] = Q[state]\n",
    "            \n",
    "    for key, value in _Q.items():\n",
    "        string = ','.join([str(k) for k in key]) + ',' + ','.join([str(v) for v in value]) + '\\n'\n",
    "        f.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = Env()\n",
    "nA = env.nA\n",
    "nS = env.nS\n",
    "header = ','.join(['s{}'.format(k_) for k_ in range(nS)]) + ',' \\\n",
    "                + ','.join(['v{}'.format(k_) for k_ in range(nA)]) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = Env()\n",
    "nA = env.nA\n",
    "nS = env.nS\n",
    "with open('./Q.csv', 'w') as f:\n",
    "    header = ','.join(['s{}'.format(k_) for k_ in range(nS)]) + ',' \\\n",
    "                + ','.join(['v{}'.format(k_) for k_ in range(nA)]) + '\\n'\n",
    "    f.write(header)\n",
    "    for key, value in Q.items():\n",
    "        string = ','.join([str(k) for k in key]) + ',' + ','.join([str(v) for v in value]) + '\\n'\n",
    "        f.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datsv = pd.read_csv('./Q.csv').astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, env, scope='actor'):\n",
    "        self._num_input = env.nS\n",
    "        self._num_output = env.nA\n",
    "        with tf.variable_scope(scope):\n",
    "            self._x = tf.placeholder(dtype=tf.float32, shape=[None, self._num_input], name='x')\n",
    "            self._y = tf.placeholder(dtype=tf.float32, shape=[None, self._num_output], name='y')\n",
    "            self._training = tf.placeholder_with_default(True, shape=(), name='training')\n",
    "\n",
    "            o0 = tf.layers.dense(self._x, 64, activation=tf.nn.relu, name='output-0')\n",
    "            d0 = tf.layers.dropout(o0, rate=0.7, training=self._training, name='dropout-0')\n",
    "            o1 = tf.layers.dense(d0, 64, activation=tf.nn.relu, name='output-1')\n",
    "            d1 = tf.layers.dropout(o1, rate=0.7, training=self._training, name='dropout-1')\n",
    "            o2 = tf.layers.dense(d1, 64, activation=tf.nn.relu, name='output-2')\n",
    "            d2 = tf.layers.dropout(o2, rate=0.7, training=self._training, name='dropout-2')\n",
    "            self._pred = tf.layers.dense(d2, self._num_output, activation=tf.nn.softmax, name='pred')\n",
    "\n",
    "            self._loss = tf.nn.softmax_cross_entropy_with_logits(labels=self._y, logits=self._pred)\n",
    "            self._op = tf.train.AdamOptimizer(learning_rate=.01).minimize(self._loss)\n",
    "\n",
    "    def train(self, sess, x, y):\n",
    "        assert isinstance(sess, tf.Session)\n",
    "        loss, _ = sess.run([self._loss, self._op], feed_dict={self._x: x, self._y: y, self._training: True})\n",
    "        return loss\n",
    "\n",
    "    def predict(self, sess, x):\n",
    "        assert isinstance(sess, tf.Session)\n",
    "        return sess.run(self._pred, feed_dict={self._x: x, self._training: False})\n",
    "\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, env, scope='critic'):\n",
    "        self._num_input = env.nS\n",
    "        self._num_output = env.nA\n",
    "        with tf.variable_scope(scope):\n",
    "            self._x = tf.placeholder(dtype=tf.float32, shape=[None, self._num_input], name='x')\n",
    "            self._y = tf.placeholder(dtype=tf.float32, shape=[None, self._num_output], name='y')\n",
    "            self._action = tf.placeholder(dtype=tf.int32, shape=[None, 1], name='action')\n",
    "            self._training = tf.placeholder_with_default(True, shape=(), name='training')\n",
    "\n",
    "            this_batch_size = tf.shape(self._x)[0]\n",
    "            o0 = tf.layers.dense(self._x, 64, activation=tf.nn.relu, name='output-0')\n",
    "            d0 = tf.layers.dropout(o0, rate=0.7, training=self._training, name='dropout-0')\n",
    "            o1 = tf.layers.dense(d0, 64, activation=tf.nn.relu, name='output-1')\n",
    "            d1 = tf.layers.dropout(o1, rate=0.7, training=self._training, name='dropout-1')\n",
    "            o2 = tf.layers.dense(d1, 64, activation=tf.nn.relu, name='output-2')\n",
    "            d2 = tf.layers.dropout(o2, rate=0.7, training=self._training, name='dropout-2')\n",
    "            self._pred = tf.layers.dense(d2, self._num_output, activation=None, name='pred')\n",
    "\n",
    "            indices = tf.range(this_batch_size, dtype=tf.int32) * self._num_output + tf.squeeze(self._action)\n",
    "            preds = tf.reshape(tf.gather(tf.reshape(self._pred, shape=[-1]), indices), shape=[1, this_batch_size])\n",
    "            y = tf.reshape(tf.gather(tf.reshape(self._y, shape=[-1]), indices), shape=[1, this_batch_size])\n",
    "            \n",
    "            #define pre-train methods\n",
    "            self._pre_train_loss = tf.losses.mean_squared_error(self._y, self._pred)\n",
    "            self._pre_train_op = tf.train.AdamOptimizer(0.1).minimize(self._pre_train_loss)\n",
    "            \n",
    "            self._loss = tf.losses.mean_squared_error(y, preds)\n",
    "            self._op = tf.train.AdamOptimizer(0.1).minimize(self._loss)\n",
    "\n",
    "    def train(self, sess, x, y, action):\n",
    "        assert isinstance(sess, tf.Session)\n",
    "        loss, _ = sess.run([self._loss, self._op], feed_dict={self._x: x, self._y: y, self._action: action, self._training: True})\n",
    "        return loss\n",
    "\n",
    "    def predict(self, sess, x):\n",
    "        assert isinstance(sess, tf.Session)\n",
    "        return sess.run(self._pred, feed_dict={self._x: x, self._training:False})\n",
    "    \n",
    "    def pre_train(self, sess, x, y):\n",
    "        assert isinstance(sess, tf.Session)\n",
    "        loss, _ = sess.run([self._pre_train_loss, self._pre_train_op], feed_dict={self._x: x, self._y: y})\n",
    "        return loss\n",
    "    \n",
    "    def pre_train_loss(self, sess, x, y):\n",
    "        assert isinstance(sess, tf.Session)\n",
    "        return sess.run(self._pre_train_loss, feed_dict={self._x: x, self._y: y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "env = Env()\n",
    "nS = env.nS\n",
    "nA = env.nA\n",
    "critic = Critic(env)\n",
    "num_row = len(datsv)\n",
    "batch_size = 20000\n",
    "batch_num = int((num_row / batch_size) + 1)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _train(d):\n",
    "    dat = d.sample(frac=1.)\n",
    "    \n",
    "    for batch_idx in range(batch_num):\n",
    "        batch = dat.iloc[batch_idx * batch_size: (batch_idx + 1) * batch_size, :]\n",
    "        batch_x = batch.iloc[:, :nS]\n",
    "        batch_y = batch.iloc[:, nS:]\n",
    "        critic.pre_train(sess, batch_x, batch_y)\n",
    "    return critic.pre_train_loss(sess, dat.iloc[:, :nS], dat.iloc[:, nS:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoc_idx in range(50):\n",
    "    print('\\repoc no. {:>10}'.format(epoc_idx), end='')\n",
    "    losses.append(_train(datsv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the trained critic network\n",
    "\n",
    "def get_action(state):\n",
    "    state = np.array(state).reshape([1, -1])\n",
    "    preds = critic.predict(sess=sess, x=state)\n",
    "    return np.argmax(preds)\n",
    "\n",
    "env = Env(tweaked_times=3, tweaked_times_limit=6)\n",
    "steps = []\n",
    "for _ in  range(100):\n",
    "    state = env.reset()\n",
    "    for t in itertools.count():\n",
    "        action = get_action(state)\n",
    "        state_prime, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            steps.append(t)\n",
    "            break\n",
    "        else:\n",
    "            state = state_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#epsilon = .05\n",
    "epsilon = 0\n",
    "def get_action(state):\n",
    "    \"\"\"\n",
    "    保证策略可以在贪婪性上做出变化\n",
    "    此处的贪婪系数为0，是因为在这种环境下，贪婪策略不会导致智能体永远选择 次好的 动作(随着魔方初始旋转次数的增加，可以考虑改变贪婪系数探索更优策略的可能)\n",
    "    \"\"\"\n",
    "    probs = np.ones(nA) * (epsilon / nA)\n",
    "    probs[np.argmax(Q[state])] += (1. - epsilon)\n",
    "    return np.random.choice(np.arange(nA), p=probs)\n",
    "\n",
    "for n in range(0, 6): # n代表了初始化的魔方的旋转次数（如果直接选择较大的旋转次数，会增加训练的智能体的迷茫）\n",
    "    for i in range(5 * (6 ** n)): # 5 * 6 ** n 稍微加强了智能体遍历各种魔方状态的可能（但是泛化能力几乎为零，这种方法类似于MC方法的暴力美学）\n",
    "        # implement Sarsa with one Q in the above cell\n",
    "        env = Env(tweaked_times=n, tweaked_times_limit=n * 2)\n",
    "        num_episode = 100 # 一种初始状态下的最多的尝试次数\n",
    "\n",
    "        discounter = 0.9 # temporal difference (TD) 中的折扣系数 lambda 参见 贝尔曼方程 bellman equation https://en.wikipedia.org/wiki/Bellman_equation\n",
    "        sum_rewards = deque(maxlen=30)\n",
    "        avg_sum_rewards = []\n",
    "        nA = env.nA\n",
    "\n",
    "        for episode_idx in range(num_episode):\n",
    "            state = env.reset()\n",
    "            action = get_action(state)\n",
    "            \n",
    "            td_errors = 0 # Q-V 的误差累积值\n",
    "\n",
    "            rewards = 0\n",
    "            for t in itertools.count():\n",
    "                \n",
    "                state_prime, reward, done, _ = env.step(action)\n",
    "                rewards += reward\n",
    "                action_prime = get_action(state_prime)\n",
    "                td_error = reward + discounter * Q[state_prime][action_prime] - Q[state][action] # 参见 https://en.wikipedia.org/wiki/Temporal_difference_learning\n",
    "                Q[state][action] += td_error * 0.3 # 0.3是学习率 eta\n",
    "                \n",
    "                td_errors += np.abs(td_error)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "                else:\n",
    "                    state = state_prime\n",
    "                    action = action_prime\n",
    "            sum_rewards.append(rewards)\n",
    "            avg_sum_rewards.append(np.mean(sum_rewards))\n",
    "            \n",
    "            print('\\r {} {} {} {:>30}'.format(n, i, episode_idx, td_errors), end='')\n",
    "            if td_errors < 10 ** -5: # 如果累积的误差已经很小的话，没有必要再持续尝试（state value近乎收敛）\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implement Sarsa with one Q in the above cell\n",
    "env = Env(tweaked_times=5)\n",
    "num_episode = 100\n",
    "#epsilon = .05\n",
    "epsilon = 0\n",
    "discounter = 0.9\n",
    "sum_rewards = deque(maxlen=30)\n",
    "avg_sum_rewards = []\n",
    "nA = env.nA\n",
    "\n",
    "def get_action(state):\n",
    "    probs = np.ones(nA) * (epsilon / nA)\n",
    "    probs[np.argmax(Q[state])] += (1. - epsilon)\n",
    "    return np.random.choice(np.arange(nA), p=probs)\n",
    "\n",
    "for episode_idx in range(num_episode):\n",
    "    state = env.reset()\n",
    "    action = get_action(state)\n",
    "\n",
    "    rewards = 0\n",
    "    for t in itertools.count():\n",
    "        state_prime, reward, done, _ = env.step(action)\n",
    "        rewards += reward\n",
    "        action_prime = get_action(state_prime)\n",
    "        Q[state][action] += (reward + discounter * Q[state_prime][action_prime] - Q[state][action]) * 0.3\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            state = state_prime\n",
    "            action = action_prime\n",
    "    sum_rewards.append(rewards)\n",
    "    avg_sum_rewards.append(np.mean(sum_rewards))\n",
    "\n",
    "\n",
    "plt.plot(avg_sum_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q = defaultdict(lambda :np.zeros(len(action_space)))\n",
    "env = Env()\n",
    "num_episode = 150\n",
    "#epsilon = .05\n",
    "epsilon = 0\n",
    "discounter = 0.9\n",
    "sum_rewards = deque(maxlen=30)\n",
    "avg_sum_rewards = []\n",
    "nA = env.nA\n",
    "\n",
    "def get_action(state):\n",
    "    probs = np.ones(nA) * (epsilon / nA)\n",
    "    probs[np.argmax(Q[state])] += (1. - epsilon)\n",
    "    return np.random.choice(np.arange(nA), p=probs)\n",
    "\n",
    "for episode_idx in range(num_episode):\n",
    "    state = env.reset()\n",
    "    action = get_action(state)\n",
    "    \n",
    "    rewards = 0\n",
    "    for t in itertools.count():\n",
    "        state_prime, reward, done, _ = env.step(action)\n",
    "        rewards += reward\n",
    "        action_prime = get_action(state_prime)\n",
    "        Q[state][action] += (reward + discounter * Q[state_prime][action_prime] - Q[state][action]) * 0.3\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            state = state_prime\n",
    "            action = action_prime\n",
    "    sum_rewards.append(rewards)\n",
    "    avg_sum_rewards.append(np.mean(sum_rewards))\n",
    "plt.plot(avg_sum_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q = defaultdict(lambda :np.zeros([len(action_space), 2]))\n",
    "env = Env()\n",
    "num_episode = 150\n",
    "#epsilon = .05\n",
    "epsilon = 0\n",
    "discounter = 0.9\n",
    "nA = env.nA\n",
    "sum_rewards = deque(maxlen=30)\n",
    "avg_sum_rewards1 = []\n",
    "\n",
    "def get_action(state):\n",
    "    probs = np.ones(nA) * (epsilon / nA)\n",
    "    probs[np.argmax(Q[state][:, 0])] += (1. - epsilon)\n",
    "    return np.random.choice(np.arange(nA), p=probs)\n",
    "\n",
    "for episode_idx in range(num_episode):\n",
    "    state = env.reset()\n",
    "    rewards = 0\n",
    "    for t in itertools.count():\n",
    "        action = get_action(state)\n",
    "        state_prime, reward, done, _ = env.step(action)\n",
    "        rewards += reward\n",
    "        \n",
    "        action_value = reward + discounter * np.max(np.squeeze(Q[state_prime][:, 0]))\n",
    "        Q[state][action][0] += (action_value - Q[state][action][0]) / (Q[state][action][1] + 1)\n",
    "        Q[state][action][1] += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            state = state_prime\n",
    "    sum_rewards.append(rewards)\n",
    "    avg_sum_rewards1.append(np.mean(sum_rewards))\n",
    "plt.plot(avg_sum_rewards1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q = defaultdict(lambda :np.zeros(len(action_space)))\n",
    "env = Env()\n",
    "num_episode = 150\n",
    "#epsilon = .05\n",
    "epsilon = 0\n",
    "discounter = 0.9\n",
    "nA = env.nA\n",
    "sum_rewards = deque(maxlen=30)\n",
    "avg_sum_rewards2 = []\n",
    "\n",
    "def get_action(state):\n",
    "    probs = np.ones(nA) * (epsilon / nA)\n",
    "    probs[np.argmax(Q[state])] += (1. - epsilon)\n",
    "    return np.random.choice(np.arange(nA), p=probs)\n",
    "\n",
    "for episode_idx in range(num_episode):\n",
    "    state = env.reset()\n",
    "    rewards = 0\n",
    "    for t in itertools.count():\n",
    "        action = get_action(state)\n",
    "        state_prime, reward, done, _ = env.step(action)\n",
    "        rewards += reward\n",
    "        \n",
    "        action_value = reward + discounter * np.max(np.squeeze(Q[state_prime]))\n",
    "        Q[state][action] += (action_value - Q[state][action]) * .3\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            state = state_prime\n",
    "    sum_rewards.append(rewards)\n",
    "    avg_sum_rewards2.append(np.mean(sum_rewards))\n",
    "plt.plot(avg_sum_rewards2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(avg_sum_rewards1, label='Q-learning_avg')\n",
    "plt.plot(avg_sum_rewards, label='Sarsa')\n",
    "plt.plot(avg_sum_rewards2, label='Q-learning_discount')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q[env.reset()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_space[np.argmax(Q[env.reset()][:, 0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.cube(str(action_space[np.argmax(Q[env.reset()][:, 0])])).check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = Env()\n",
    "ret = []\n",
    "for _ in range(1200):\n",
    "    env.reset()\n",
    "    env.cube(str(np.random.choice(action_space)))\n",
    "    ret.append(env.cube.check())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.sum(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_episode = 3000\n",
    "memory = deque(maxlen=100)\n",
    "tf.reset_default_graph()\n",
    "env = Env()\n",
    "actor, critic = Actor(env), Critic(env)\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "rewards = []\n",
    "loss_critic = []\n",
    "\n",
    "rewards_deque = deque(maxlen=80)\n",
    "epsilons = np.linspace(start=.5, stop=.01, num=num_episode + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for episode_idx in range(num_episode):\n",
    "    epsilon = epsilons[episode_idx]\n",
    "    state = env.reset()\n",
    "    for t in itertools.count():\n",
    "        probs = np.ones(env.nA) * (epsilon / env.nA)\n",
    "        probs[np.argmax(np.squeeze(actor.predict(session, np.array(state).reshape(1, env.nS))))] += (1. - epsilon)\n",
    "        action = np.random.choice(np.arange(env.nA), p=probs)\n",
    "        \n",
    "        state_prime, reward, done, _ = env.step(action)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        rewards_deque.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            memory.append((state, action, state_prime, reward))\n",
    "        \n",
    "            if len(memory) >= memory.maxlen:\n",
    "                print('\\r in episode #{0}, step #{1}, avg score {2:>8}'.format(episode_idx, t, np.mean(rewards_deque)), end='')\n",
    "                sys.stdout.flush()\n",
    "                rp = np.array(memory)[np.random.randint(memory.maxlen, size=50), :]\n",
    "                state_prime_batch = np.array([a for a in rp[:, 2]])\n",
    "                reward_batch = rp[:, 3].reshape(-1, 1)\n",
    "                state_batch = np.array([a for a in rp[:, 0]])\n",
    "                action_batch = rp[:, 1].reshape(-1, 1)\n",
    "\n",
    "                state_prime_action_values = critic.predict(session, state_prime_batch)\n",
    "                td_target = reward_batch + .8 * state_prime_action_values\n",
    "                td_error = td_target - critic.predict(session, state_batch)\n",
    "\n",
    "                l_actor = actor.train(session, state_batch, td_error)\n",
    "                l_critic = critic.train(session, state_batch, td_target, action_batch)\n",
    "                \n",
    "                loss_critic.append(l_critic)\n",
    "                \n",
    "            state = state_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_critic)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
